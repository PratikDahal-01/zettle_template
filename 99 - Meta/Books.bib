@article{ActorModel2025,
  title = {Actor Model},
  year = {2025},
  month = may,
  journal = {Wikipedia},
  urldate = {2025-05-06},
  abstract = {The actor model in computer science is a mathematical model of concurrent computation that treats an actor as the basic building block of concurrent computation. In response to a message it receives, an actor can: make local decisions, create more actors, send more messages, and determine how to respond to the next message received. Actors may modify their own private state, but can only affect each other indirectly through messaging (removing the need for lock-based synchronization). The actor model originated in 1973. It has been used both as a framework for a theoretical understanding of computation and as the theoretical basis for several practical implementations of concurrent systems. The relationship of the model to other work is discussed in actor model and process calculi.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1288281529},
  file = {C:\Users\Dell\Zotero\storage\75K7C3P3\Actor_model.html}
}

@article{ahrensHowTakeSmart,
  title = {How to {{Take Smart Notes}}: {{One Simple Technique}} to {{Boost Writing}}, {{Learning}} and {{Thinking}} -- for {{Students}}, {{Academics}} and {{Nonfiction Book Writers}}},
  author = {Ahrens, S{\"o}nke},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\2XXRCUF7\Ahrens - How to Take Smart Notes One Simple Technique to Boost Writing, Learning and Thinking – for Students.pdf}
}

@book{antigaDeepLearningPyTorch2020,
  title = {Deep Learning with {{PyTorch}}},
  author = {Antiga, Luca Pietro Giovanni},
  year = {2020},
  publisher = {Manning},
  address = {Shelter Island, NY},
  abstract = {Intro -- Copyright -- dedication -- contents -- front matter -- foreword -- preface -- acknowledgments -- about this book -- Who should read this book -- How this book is organized: A roadmap -- About the code -- Hardware and software requirements -- liveBook discussion forum -- Other online resources -- about the authors -- about the cover illustration -- Part 1. Core PyTorch -- 1 Introducing deep learning and the PyTorch Library -- 1.1 The deep learning revolution -- 1.2 PyTorch for deep learning -- 1.3 Why PyTorch? -- 1.3.1 The deep learning competitive landscape -- 1.4 An overview of how PyTorch supports deep learning projects -- 1.5 Hardware and software requirements -- 1.5.1 Using Jupyter Notebooks -- 1.6 Exercises -- 1.7 Summary -- 2 Pretrained networks -- 2.1 A pretrained network that recognizes the subject of an image -- 2.1.1 Obtaining a pretrained network for image recognition -- 2.1.2 AlexNet -- 2.1.3 ResNet -- 2.1.4 Ready, set, almost run -- 2.1.5 Run! -- 2.2 A pretrained model that fakes it until it makes it -- 2.2.1 The GAN game -- 2.2.2 CycleGAN -- 2.2.3 A network that turns horses into zebras -- 2.3 A pretrained network that describes scenes -- 2.3.1 NeuralTalk2 -- 2.4 Torch Hub -- 2.5 Conclusion -- 2.6 Exercises -- 2.7 Summary -- 3 It starts with a tensor -- 3.1 The world as floating-point numbers -- 3.2 Tensors: Multidimensional arrays -- 3.2.1 From Python lists to PyTorch tensors -- 3.2.2 Constructing our first tensors -- 3.2.3 The essence of tensors -- 3.3 Indexing tensors -- 3.4 Named tensors -- 3.5 Tensor element types -- 3.5.1 Specifying the numeric type with dtype -- 3.5.2 A dtype for every occasion -- 3.5.3 Managing a tensor's dtype attribute -- 3.6 The tensor API -- 3.7 Tensors: Scenic views of storage -- 3.7.1 Indexing into storage -- 3.7.2 Modifying stored values: In-place operations},
  collaborator = {Stevens, Eli and Viehmann, Thomas},
  isbn = {978-1-61729-526-3 978-1-63835-407-9},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\XLUDHV3J\Antiga - 2020 - Deep learning with PyTorch.pdf}
}

@article{barryHeadFirstPython,
  title = {Head {{First Python}}},
  author = {Barry, Paul},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\Q5YHMJXA\Barry - Head First Python.pdf}
}

@book{brousseauLLMsProductionLanguage2025,
  title = {{{LLMs}} in Production: From Language Models to Successful Products},
  shorttitle = {{{LLMs}} in Production},
  author = {Brousseau, Christopher and Sharp, Matthew},
  year = {2025},
  edition = {First edition},
  publisher = {Manning Publications},
  address = {Shelter Island, NY},
  abstract = {Learn how to put Large Language Model-based applications into production safely and efficiently. This practical book offers clear, example-rich explanations of how LLMs work, how you can interact with them, and how to integrate LLMs into your own applications. Find out what makes LLMs so different from traditional software and ML, discover best practices for working with them out of the lab, and dodge common pitfalls with experienced advice. In LLMs in Production you will: Grasp the fundamentals of LLMs and the technology behind them Evaluate when to use a premade LLM and when to build your own Efficiently scale up an ML platform to handle the needs of LLMs Train LLM foundation models and finetune an existing LLM Deploy LLMs to the cloud and edge devices using complex architectures like PEFT and LoRA Build applications leveraging the strengths of LLMs while mitigating their weaknesses LLMs in Production delivers vital insights into delivering MLOps so you can easily and seamlessly guide one to production usage. Inside, you'll find practical insights into everything from acquiring an LLM-suitable training dataset, building a platform, and compensating for their immense size. Plus, tips and tricks for prompt engineering, retraining and load testing, handling costs, and ensuring security. About the Technology Most business software is developed and improved iteratively, and can change significantly even after deployment. By contrast, because LLMs are expensive to create and difficult to modify, they require meticulous upfront planning, exacting data standards, and carefully-executed technical implementation. Integrating LLMs into production products impacts every aspect of your operations plan, including the application lifecycle, data pipeline, compute cost, security, and more. Get it wrong, and you may have a costly failure on your hands. About the Book LLMs in Production teaches you how to develop an LLMOps plan that can take an AI app smoothly from design to delivery. You'll learn techniques for preparing an LLM dataset, cost-efficient training hacks like LORA and RLHF, and industry benchmarks for model evaluation. Along the way, you'll put your new skills to use in three exciting example projects: creating and training a custom LLM, building a VSCode AI coding extension, and deploying a small model to a Raspberry Pi. What's Inside Balancing cost and performance Retraining and load testing Optimizing models for commodity hardware Deploying on a Kubernetes cluster About the Reader For data scientists and ML engineers who know Python and the basics of cloud deployment. About the Authors Christopher Brousseau and Matt Sharp are experienced engineers who have led numerous successful large scale LLM deployments. Quotes Covers all the essential aspects of how to build and deploy LLMs. It goes into the deep and fascinating areas that most other books gloss over. - Andrew Carr, Cartwheel A must-read for anyone looking to harness the potential of LLMs in production environments. - Jepson Taylor, VEOX Inc. An exceptional guide that simplifies the building and deployment of complex LLMs. - Arunkumar Gopalan, Microsoft UK A thorough and practical guide for running LLMs in production. - Dinesh Chitlangia, AMD},
  isbn = {978-1-63343-720-3},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\7ZYMLETL\Brousseau and Sharp - 2025 - LLMs in production from language models to successful products.pdf}
}

@book{danjouSeriousPythonBlackbelt2019,
  title = {Serious {{Python}}: Black-Belt Advice on Deployment, Scalability, Testing, and More},
  shorttitle = {Serious {{Python}}},
  author = {Danjou, Julien},
  year = {2019},
  publisher = {No Starch Press, Inc},
  address = {San Francisco, CA},
  abstract = {"Offers experienced coders advice and tips for improving knowledge of Python coding language. Includes interviews with Python experts and covers a wide range of common topics, from scaling and testing code to designing APIs"--},
  isbn = {978-1-59327-878-6},
  langid = {english},
  lccn = {QA76.73.P98 D36 2019},
  keywords = {Python (Computer program language)},
  file = {C:\Users\Dell\Zotero\storage\CXD2WJI3\Danjou - 2019 - Serious Python black-belt advice on deployment, scalability, testing, and more.pdf}
}

@misc{daoFlashAttentionFastMemoryEfficient2022,
  title = {{{FlashAttention}}: {{Fast}} and {{Memory-Efficient Exact Attention}} with {{IO-Awareness}}},
  shorttitle = {{{FlashAttention}}},
  author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  year = {2022},
  month = jun,
  number = {arXiv:2205.14135},
  eprint = {2205.14135},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.14135},
  urldate = {2025-03-26},
  abstract = {Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IOaware---accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15\% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3 speedup on GPT-2 (seq. length 1K), and 2.4 speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4\% accuracy) and Path-256 (seq. length 64K, 63.1\% accuracy).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\Dell\Zotero\storage\XRHVJ3IS\Dao et al. - 2022 - FlashAttention Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf}
}

@book{deidaWaySuperiorMan2019,
  title = {The Way of the Superior Man: A Spiritual Guide to Mastering the Challenges of Women, Work, and Sexual Desire},
  shorttitle = {The Way of the Superior Man},
  author = {Deida, David},
  year = {2019},
  edition = {20th Anniversary Edition},
  publisher = {Sounds True},
  address = {Louisville, United States},
  isbn = {978-1-62203-832-9},
  langid = {english},
  annotation = {OCLC: 1380457131},
  file = {C:\Users\Dell\Zotero\storage\LXTHTCA6\Deida - 2019 - The way of the superior man a spiritual guide to mastering the challenges of women, work, and sexua.pdf}
}

@article{deviPuzzlesPuzzleYou,
  title = {Puzzles to {{Puzzle You}}},
  author = {Devi, Shakuntala},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\AEWZRV99\Devi - Puzzles to Puzzle You.pdf}
}

@article{domingosFewUsefulThings2012,
  title = {A Few Useful Things to Know about Machine Learning},
  author = {Domingos, Pedro},
  year = {2012},
  month = oct,
  journal = {Communications of the ACM},
  volume = {55},
  number = {10},
  pages = {78--87},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/2347736.2347755},
  urldate = {2025-05-15},
  abstract = {Tapping into the "folk knowledge" needed to advance machine learning applications.},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\9P2M6PE5\Domingos - 2012 - A few useful things to know about machine learning.pdf}
}

@article{foxComputerArchitecture,
  title = {Computer {{Architecture}}},
  author = {Fox, Charles},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\NS8TA6QS\Fox - Computer Architecture.pdf}
}

@book{gladwellOutliersStorySuccess2011,
  title = {Outliers: The Story of Success},
  shorttitle = {Outliers},
  author = {Gladwell, Malcolm},
  year = {2011},
  edition = {1st, Back Bay paperb. ed},
  publisher = {{Back Bay Books Little, Brown and Company}},
  address = {New York, NY Boston MA London},
  isbn = {978-0-316-01792-3 978-0-316-01793-0},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\N5JFCXQ6\Gladwell - 2011 - Outliers the story of success.pdf}
}

@article{godoyDeepLearningPyTorch,
  title = {Deep {{Learning}} with {{PyTorch Step-by-Step}}: {{A Beginner}}'s {{Guide}}},
  author = {Godoy, Daniel Voigt},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\DI6L9Z96\Godoy - Deep Learning with PyTorch Step-by-Step A Beginner’s Guide.pdf}
}

@article{heehlerWellspokenThesaurus,
  title = {Well-Spoken {{Thesaurus}}},
  author = {Heehler, Tom},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\L6SNIMPT\Heehler - Well-spoken Thesaurus.pdf}
}

@misc{huLoRALowRankAdaptation2021,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and {Allen-Zhu}, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  year = {2021},
  month = oct,
  number = {arXiv:2106.09685},
  eprint = {2106.09685},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.09685},
  urldate = {2025-05-20},
  abstract = {An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\Dell\Zotero\storage\MEWGPL35\Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf}
}

@misc{IntroducingContextualRetrieval,
  title = {Introducing {{Contextual Retrieval}}},
  urldate = {2025-05-16},
  abstract = {Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.},
  howpublished = {https://www.anthropic.com/news/contextual-retrieval},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\37NHU6A7\contextual-retrieval.html}
}

@book{irvineGuideGoodLife2009,
  title = {A Guide to the Good Life: The Ancient Art of {{Stoic}} Joy},
  shorttitle = {A Guide to the Good Life},
  author = {Irvine, William Braxton},
  year = {2009},
  publisher = {Oxford University Press},
  address = {Oxford ; New York},
  isbn = {978-0-19-537461-2},
  langid = {english},
  lccn = {B528 .I78 2009},
  keywords = {Stoics},
  annotation = {OCLC: ocn213495186},
  file = {C:\Users\Dell\Zotero\storage\HE9HA6RR\Irvine - 2009 - A guide to the good life the ancient art of Stoic joy.pdf}
}

@misc{iyerOPTIMLScalingLanguage2023,
  title = {{{OPT-IML}}: {{Scaling Language Model Instruction Meta Learning}} through the {{Lens}} of {{Generalization}}},
  shorttitle = {{{OPT-IML}}},
  author = {Iyer, Srinivasan and Lin, Xi Victoria and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, Daniel and Yu, Ping and Shuster, Kurt and Wang, Tianlu and Liu, Qing and Koura, Punit Singh and Li, Xian and O'Horo, Brian and Pereyra, Gabriel and Wang, Jeff and Dewan, Christopher and Celikyilmaz, Asli and Zettlemoyer, Luke and Stoyanov, Ves},
  year = {2023},
  month = jan,
  number = {arXiv:2212.12017},
  eprint = {2212.12017},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.12017},
  urldate = {2025-05-27},
  abstract = {Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-IML Bench: a large benchmark for Instruction MetaLearning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, and prepare an evaluation framework to measure three types of model generalizations: to tasks from fully held-out categories, to held-out tasks from seen categories, and to held-out instances from seen tasks. Through the lens of this framework, we first present insights about instructiontuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B, which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats -- PromptSource, FLAN, Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on each specific benchmark. We release OPT-IML at both scales, together with the OPT-IML Bench evaluation framework.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\Dell\Zotero\storage\Q2I4V6HM\Iyer et al. - 2023 - OPT-IML Scaling Language Model Instruction Meta Learning through the Lens of Generalization.pdf}
}

@book{kalbObjectorientedPythonMaster2022,
  title = {Object-Oriented {{Python}}: Master {{OOP}} by Building Games and {{GUIs}}},
  shorttitle = {Object-Oriented {{Python}}},
  author = {Kalb, Irv},
  year = {2022},
  publisher = {No Starch Press},
  address = {San Francisco},
  abstract = {"This guide to mastering object-oriented programming with Python covers the basics of building classes and creating objects through clear examples using the pygame package. Also explores key concepts like encapsulation, polymorphism, and inheritance as well as best practices for coding with objects"--},
  isbn = {978-1-7185-0206-2},
  langid = {english},
  lccn = {QA76.64 .K3563 2022},
  keywords = {Object-oriented programming (Computer science),Python (Computer program language)},
  file = {C:\Users\Dell\Zotero\storage\XLIF2LYD\Kalb - 2022 - Object-oriented Python master OOP by building games and GUIs.pdf}
}

@book{kennedyMetasploit2ndEdition2025,
  title = {Metasploit, 2nd {{Edition}}},
  author = {Kennedy, David},
  year = {2025},
  edition = {1st ed},
  publisher = {No Starch Press},
  address = {New York},
  collaborator = {Aharoni, Mati and Kearns, Devon and O'Gorman, Jim and Graham, Daniel G.},
  isbn = {978-1-7185-0299-4},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\QDZKXZE7\Kennedy - 2025 - Metasploit, 2nd Edition.epub}
}

@article{kerrPRACTICALUSERS,
  title = {A {{PR ACTICAL USER}}'{{S GUIDE TO WORKING SMARTER ON THE COMMAND LINE}}},
  author = {Kerr, Dave},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\MEYECNDM\Kerr - A PR ACTICAL USER’S GUIDE TO WORKING SMARTER ON THE COMMAND LINE.pdf}
}

@article{klabnikRustProgrammingLanguage,
  title = {The {{Rust Programming Language}}},
  author = {Klabnik, Steve and Nichols, Carol},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\X5CJNQ5K\Klabnik and Nichols - The Rust Programming Language.pdf}
}

@article{kneuselMathDeepLearning,
  title = {Math for {{Deep Learning}}},
  author = {Kneusel, Ronald T},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\ETW97WZ8\Kneusel - Math for Deep Learning.pdf}
}

@book{kneuselMathProgramming2025,
  title = {Math for {{Programming}}},
  author = {Kneusel, Ronald T.},
  year = {2025},
  month = feb,
  journal = {urn:isbn:9781718503595},
  publisher = {No Starch Press, Inc.},
  copyright = {Copyright {\copyright} 2025 by Ronald T. Kneusel.},
  isbn = {978-1-7185-0359-5},
  langid = {english},
  annotation = {Item ID: \_:n0},
  file = {C:\Users\Dell\Zotero\storage\MYAF32BA\Kneusel - 2025 - Math for Programming.epub}
}

@misc{KrutrimailabsKrutrim2instructHugging,
  title = {Krutrim-Ai-Labs/{{Krutrim-2-instruct}} {$\cdot$} {{Hugging Face}}},
  urldate = {2025-03-27},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/krutrim-ai-labs/Krutrim-2-instruct},
  file = {C:\Users\Dell\Zotero\storage\3L67VNZ9\Krutrim-2-instruct.html}
}

@article{kubicaDataStructuresFun,
  title = {Data {{Structures}} the {{Fun Way}}},
  author = {Kubica, Jeremy},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\KQPD8FSB\Kubica - Data Structures the Fun Way.pdf}
}

@misc{liuTrainingFreeActivationSparsity2025,
  title = {Training-{{Free Activation Sparsity}} in {{Large Language Models}}},
  author = {Liu, James and Ponnusamy, Pragaash and Cai, Tianle and Guo, Han and Kim, Yoon and Athiwaratkun, Ben},
  year = {2025},
  month = feb,
  number = {arXiv:2408.14690},
  eprint = {2408.14690},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.14690},
  urldate = {2025-05-29},
  abstract = {Activation sparsity can enable practical inference speedups in large language models (LLMs) by reducing the compute and memory-movement required for matrix multiplications during the forward pass. However, existing methods face limitations that inhibit widespread adoption. Some approaches are tailored towards older models with ReLU-based sparsity, while others require extensive continued pre-training on up to hundreds of billions of tokens. This paper describes TEAL, a simple training-free method that applies magnitude-based activation sparsity to hidden states throughout the entire model. TEAL achieves 40-50\% model-wide sparsity with minimal performance degradation across Llama-2, Llama-3, and Mistral families, with sizes varying from 7B to 70B. We improve existing sparse kernels and demonstrate wall-clock decoding speed-ups of up to 1.53\${\textbackslash}times\$ and 1.8\${\textbackslash}times\$ at 40\% and 50\% model-wide sparsity. TEAL is compatible with weight quantization, enabling further efficiency gains.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\Dell\\Zotero\\storage\\SJA6EF72\\Liu et al. - 2025 - Training-Free Activation Sparsity in Large Language Models.pdf;C\:\\Users\\Dell\\Zotero\\storage\\PQAJJAHE\\2408.html}
}

@article{liuTransformerbasedRealtimeLiDAR2024,
  title = {A Transformer-Based Real-Time {{LiDAR}} Semantic Segmentation Method for Restricted Mobile Devices},
  author = {Liu, Chang and Zhao, Jin and Sun, Nianyi},
  year = {2024},
  month = mar,
  journal = {Journal of the Franklin Institute},
  volume = {361},
  number = {4},
  pages = {106632},
  issn = {0016-0032},
  doi = {10.1016/j.jfranklin.2024.01.033},
  urldate = {2025-03-27},
  abstract = {In scene understanding, LiDAR-based semantic segmentation is a crucial task for describing object boundaries and sizes. Simultaneously, real-time characteristics in autonomous driving heavily rely on 3D information for navigation. With these considerations in mind, we propose a novel LiDAR real-time semantic segmentation method, which involves projecting 3D point clouds into a spherical range image and performing segmentation using 2D convolution. Building upon the success of the Transformer on 2D images, we explore its potential on 3D point clouds. To leverage the advantages of both convolution and Transformer, we introduce the Multi-Head Self-Attention (MHSA) mechanism into LiDAR semantic segmentation, as a means to enhance 2D Convolution. This results in a lightweight model with three key insights: (i) Proposing a parallel semantic segmentation architecture by combining Transformer and convolution; (ii) Innovatively splitting channels to differentiate the Transformer and convolution branches; (iii) The concept of adaptive sliding window is introduced to enhance the relationship of edge dependency when projecting 3D point clouds into range images. We evaluate our method incrementally, both qualitatively and quantitatively, on the SemanticKITTI and SemanticPOSS datasets. The experimental results demonstrate that our proposed method achieves superior performance in 3D semantic segmentation and LiDAR mapping compared to the state-of-the-art.},
  file = {C:\Users\Dell\Zotero\storage\BLVJS5MV\S0016003224000383.html}
}

@misc{longpreResponsibleFoundationModel2025,
  title = {The {{Responsible Foundation Model Development Cheatsheet}}: {{A Review}} of {{Tools}} \& {{Resources}}},
  shorttitle = {The {{Responsible Foundation Model Development Cheatsheet}}},
  author = {Longpre, Shayne and Biderman, Stella and Albalak, Alon and Schoelkopf, Hailey and McDuff, Daniel and Kapoor, Sayash and Klyman, Kevin and Lo, Kyle and Ilharco, Gabriel and San, Nay and Rauh, Maribeth and Skowron, Aviya and Vidgen, Bertie and Weidinger, Laura and Narayanan, Arvind and Sanh, Victor and Adelani, David and Liang, Percy and Bommasani, Rishi and Henderson, Peter and Luccioni, Sasha and Jernite, Yacine and Soldaini, Luca},
  year = {2025},
  month = feb,
  number = {arXiv:2406.16746},
  eprint = {2406.16746},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.16746},
  urldate = {2025-06-02},
  abstract = {Foundation model development attracts a rapidly expanding body of contributors, scientists, and applications. To help shape responsible development practices, we introduce the Foundation Model Development Cheatsheet: a growing collection of 250+ tools and resources spanning text, vision, and speech modalities. We draw on a large body of prior work to survey resources (e.g. software, documentation, frameworks, guides, and practical tools) that support informed data selection, processing, and understanding, precise and limitation-aware artifact documentation, efficient model training, advance awareness of the environmental impact from training, careful model evaluation of capabilities, risks, and claims, as well as responsible model release, licensing and deployment practices. The process of curating this list, enabled us to review the AI development ecosystem, revealing what tools are critically missing, misused, or over-used in existing practices. We find that (i) tools for data sourcing, model evaluation, and monitoring are critically under-serving ethical and real-world needs, (ii) evaluations for model safety, capabilities, and environmental impact all lack reproducibility and transparency, (iii) text and particularly English-centric analyses continue to dominate over multilingual and multi-modal analyses, and (iv) evaluation of systems, rather than just models, is needed for capabilities to be assessed in context.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\Dell\Zotero\storage\XT8VPTLU\Longpre et al. - 2025 - The Responsible Foundation Model Development Cheatsheet A Review of Tools & Resources.pdf}
}

@misc{matarazzoSurveyLargeLanguage2025,
  title = {A {{Survey}} on {{Large Language Models}} with Some {{Insights}} on Their {{Capabilities}} and {{Limitations}}},
  author = {Matarazzo, Andrea and Torlone, Riccardo},
  year = {2025},
  month = feb,
  number = {arXiv:2501.04040},
  eprint = {2501.04040},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.04040},
  urldate = {2025-06-04},
  abstract = {The rapid advancement of artificial intelligence, particularly with the development of Large Language Models (LLMs) built on the transformer architecture, has redefined the capabilities of natural language processing. These models now exhibit remarkable performance across various language-related tasks, such as text generation, question answering, translation, and summarization, often rivaling human-like comprehension. More intriguingly, LLMs have demonstrated emergent abilities extending beyond their core functions, showing proficiency in tasks like commonsense reasoning, code generation, and arithmetic. This survey paper explores the foundational components, scaling mechanisms, and architectural strategies that drive these capabilities. Emphasizing models like GPT and LLaMA, we analyze the impact of exponential data and computational growth on LLM performance, while also addressing the trade-offs associated with scaling. We also examine LLM applications across sectors, such as healthcare, finance, education, and law, highlighting their adaptability and potential to solve domain-specific challenges.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\Dell\Zotero\storage\D8BIHZCM\Matarazzo and Torlone - 2025 - A Survey on Large Language Models with some Insights on their Capabilities and Limitations.pdf}
}

@book{matloffArtDebuggingGDB2008,
  title = {The {{Art}} of {{Debugging}} with {{GDB}}, {{DDD}}, and {{Eclipse}}},
  author = {Matloff, Norman},
  year = {2008},
  publisher = {No Starch Press},
  address = {San Francisco},
  collaborator = {Salzman, Peter Jay},
  isbn = {978-1-59327-231-9},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\4TJBAVDE\Matloff - 2008 - The Art of Debugging with GDB, DDD, and Eclipse.epub}
}

@book{mcmahonQuantumComputingExplained2008,
  title = {Quantum Computing Explained},
  author = {McMahon, David},
  year = {2008},
  publisher = {Wiley-Interscience : IEEE Computer Society},
  address = {Hoboken, N.J},
  isbn = {978-0-470-09699-4},
  langid = {english},
  lccn = {QA76.889 .M42 2008},
  keywords = {Quantum computers},
  annotation = {OCLC: ocn122424963},
  file = {C:\Users\Dell\Zotero\storage\ZEKF4WLE\McMahon - 2008 - Quantum computing explained.pdf}
}

@article{meadows365DaysSelfDiscipline,
  title = {365 {{Days With Self-Discipline}}: 365 {{Life-Altering Thoughts}} on {{Self-Control}}, {{Mental Resilience}}, and {{Success}}},
  author = {Meadows, Martin},
  langid = {english},
  file = {C\:\\Users\\Dell\\Zotero\\storage\\DCWTPMYI\\Grit.pdf;C\:\\Users\\Dell\\Zotero\\storage\\SIMF8VS2\\Meadows - 365 Days With Self-Discipline 365 Life-Altering Thoughts on Self-Control, Mental Resilience, and Su.pdf}
}

@misc{mirzadehGSMSymbolicUnderstandingLimitations2024,
  title = {{{GSM-Symbolic}}: {{Understanding}} the {{Limitations}} of {{Mathematical Reasoning}} in {{Large Language Models}}},
  shorttitle = {{{GSM-Symbolic}}},
  author = {Mirzadeh, Iman and Alizadeh, Keivan and Shahrokhi, Hooman and Tuzel, Oncel and Bengio, Samy and Farajtabar, Mehrdad},
  year = {2024},
  month = oct,
  number = {arXiv:2410.05229},
  eprint = {2410.05229},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.05229},
  urldate = {2025-06-13},
  abstract = {Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a largescale study on several state-of-the-art open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of models.Our findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and demonstrate that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is due to the fact that current LLMs are not capable of genuine logical reasoning; instead, they attempt to replicate the reasoning steps observed in their training data. When we add a single clause that appears relevant to the question, we observe significant performance drops (up to 65\%) across all state-of-the-art models, even though the added clause does not contribute to the reasoning chain needed to reach the final answer. Overall, our work provides a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C:\Users\Dell\Zotero\storage\NSN42ZGJ\Mirzadeh et al. - 2024 - GSM-Symbolic Understanding the Limitations of Mathematical Reasoning in Large Language Models.pdf}
}

@article{miyamotoBookFiveRings,
  title = {The {{Book}} of {{Five Rings}}},
  author = {Miyamoto, Musashi},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\N2RPAFU6\Miyamoto - The Book of Five Rings.pdf}
}

@book{muellerDeepLearningDummies2019,
  title = {Deep Learning for Dummies},
  author = {Mueller, John Paul and Massaron, Luca},
  year = {2019},
  series = {For Dummies},
  publisher = {John Wiley \& Sons Inc},
  address = {Hoboken, NJ},
  isbn = {978-1-119-54304-6 978-1-119-54303-9 978-1-119-54302-2},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\HHVCBISZ\Mueller and Massaron - 2019 - Deep learning for dummies.pdf}
}

@misc{NExTGPT,
  title = {{{NExT-GPT}}},
  urldate = {2025-04-21},
  howpublished = {https://next-gpt.github.io/},
  file = {C:\Users\Dell\Zotero\storage\AESUSFI8\next-gpt.github.io.html}
}

@article{nietzscheThusSpokeZarathustra,
  title = {Thus {{Spoke Zarathustra}}: {{A Book}} for {{Everyone}} and {{No One}}},
  author = {Nietzsche, Friedrich and Hollingdale, R J},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\HPIZQVAQ\Nietzsche and Hollingdale - Thus Spoke Zarathustra A Book for Everyone and No One.pdf}
}

@misc{parthasarathyUltimateGuideFineTuning2024,
  title = {The {{Ultimate Guide}} to {{Fine-Tuning LLMs}} from {{Basics}} to {{Breakthroughs}}: {{An Exhaustive Review}} of {{Technologies}}, {{Research}}, {{Best Practices}}, {{Applied Research Challenges}} and {{Opportunities}}},
  shorttitle = {The {{Ultimate Guide}} to {{Fine-Tuning LLMs}} from {{Basics}} to {{Breakthroughs}}},
  author = {Parthasarathy, Venkatesh Balavadhani and Zafar, Ahtsham and Khan, Aafaq and Shahid, Arsalan},
  year = {2024},
  month = oct,
  number = {arXiv:2408.13296},
  eprint = {2408.13296},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.13296},
  urldate = {2025-05-27},
  abstract = {This report examines the fine-tuning of Large Language Models (LLMs), integrating theoretical insights with practical applications. It outlines the historical evolution of LLMs from traditional Natural Language Processing (NLP) models to their pivotal role in AI. A comparison of fine-tuning methodologies, including supervised, unsupervised, and instruction-based approaches, highlights their applicability to different tasks. The report introduces a structured seven-stage pipeline for fine-tuning LLMs, spanning data preparation, model initialization, hyperparameter tuning, and model deployment. Emphasis is placed on managing imbalanced datasets and optimization techniques. Parameter-efficient methods like Low-Rank Adaptation (LoRA) and Half Fine-Tuning are explored for balancing computational efficiency with performance. Advanced techniques such as memory fine-tuning, Mixture of Experts (MoE), and Mixture of Agents (MoA) are discussed for leveraging specialized networks and multi-agent collaboration. The report also examines novel approaches like Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO), which align LLMs with human preferences, alongside pruning and routing optimizations to improve efficiency. Further sections cover validation frameworks, post-deployment monitoring, and inference optimization, with attention to deploying LLMs on distributed and cloud-based platforms. Emerging areas such as multimodal LLMs, fine-tuning for audio and speech, and challenges related to scalability, privacy, and accountability are also addressed. This report offers actionable insights for researchers and practitioners navigating LLM fine-tuning in an evolving landscape.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\Dell\Zotero\storage\2QMK6S4J\Parthasarathy et al. - 2024 - The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs An Exhaustive Review of Technol.pdf}
}

@article{PDFTop102025,
  title = {({{PDF}}) {{Top}} 10 Algorithms in Data Mining},
  year = {2025},
  month = feb,
  journal = {ResearchGate},
  doi = {10.1007/s10115-007-0114-2},
  urldate = {2025-05-15},
  abstract = {PDF {\textbar} This paper presents the top 10 data mining algorithms identified by the IEEE International Conference on Data Mining (ICDM) in December 2006:... {\textbar} Find, read and cite all the research you need on ResearchGate},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\BBK9L6Z9\29467751_Top_10_algorithms_in_data_mining.html}
}

@misc{phanHumanitysLastExam2025,
  title = {Humanity's {{Last Exam}}},
  author = {Phan, Long and Gatti, Alice and Han, Ziwen and Li, Nathaniel and Hu, Josephina and Zhang, Hugh and Zhang, Chen Bo Calvin and Shaaban, Mohamed and Ling, John and Shi, Sean and Choi, Michael and Agrawal, Anish and Chopra, Arnav and Khoja, Adam and Kim, Ryan and Ren, Richard and Hausenloy, Jason and Zhang, Oliver and Mazeika, Mantas and Dodonov, Dmitry and Nguyen, Tung and Lee, Jaeho and Anderson, Daron and Doroshenko, Mikhail and Stokes, Alun Cennyth and Mahmood, Mobeen and Pokutnyi, Oleksandr and Iskra, Oleg and Wang, Jessica P. and Levin, John-Clark and Kazakov, Mstyslav and Feng, Fiona and Feng, Steven Y. and Zhao, Haoran and Yu, Michael and Gangal, Varun and Zou, Chelsea and Wang, Zihan and Popov, Serguei and Gerbicz, Robert and Galgon, Geoff and Schmitt, Johannes and Yeadon, Will and Lee, Yongki and Sauers, Scott and Sanchez, Alvaro and Giska, Fabian and Roth, Marc and Riis, S{\o}ren and Utpala, Saiteja and Burns, Noah and Goshu, Gashaw M. and Naiya, Mohinder Maheshbhai and Agu, Chidozie and Giboney, Zachary and Cheatom, Antrell and {Fournier-Facio}, Francesco and Crowson, Sarah-Jane and Finke, Lennart and Cheng, Zerui and Zampese, Jennifer and Hoerr, Ryan G. and Nandor, Mark and Park, Hyunwoo and Gehrunger, Tim and Cai, Jiaqi and McCarty, Ben and Garretson, Alexis C. and Taylor, Edwin and Sileo, Damien and Ren, Qiuyu and Qazi, Usman and Li, Lianghui and Nam, Jungbae and Wydallis, John B. and Arkhipov, Pavel and Shi, Jack Wei Lun and Bacho, Aras and Willcocks, Chris G. and Cao, Hangrui and Motwani, Sumeet and Santos, Emily de Oliveira and Veith, Johannes and Vendrow, Edward and Cojoc, Doru and Zenitani, Kengo and Robinson, Joshua and Tang, Longke and Li, Yuqi and Vendrow, Joshua and Fraga, Natanael Wildner and Kuchkin, Vladyslav and Maksimov, Andrey Pupasov and Marion, Pierre and Efremov, Denis and Lynch, Jayson and Liang, Kaiqu and Mikov, Aleksandar and Gritsevskiy, Andrew and Guillod, Julien and Demir, G{\"o}zdenur and Martinez, Dakotah and Pageler, Ben and Zhou, Kevin and Soori, Saeed and Press, Ori and Tang, Henry and Rissone, Paolo and Green, Sean R. and Br{\"u}ssel, Lina and Twayana, Moon and Dieuleveut, Aymeric and Imperial, Joseph Marvin and Prabhu, Ameya and Yang, Jinzhou and Crispino, Nick and Rao, Arun and Zvonkine, Dimitri and Loiseau, Gabriel and Kalinin, Mikhail and Lukas, Marco and Manolescu, Ciprian and Stambaugh, Nate and Mishra, Subrata and Hogg, Tad and Bosio, Carlo and Coppola, Brian P. and Salazar, Julian and Jin, Jaehyeok and Sayous, Rafael and Ivanov, Stefan and Schwaller, Philippe and Senthilkuma, Shaipranesh and Bran, Andres M. and Algaba, Andres and den Houte, Kelsey Van and Sypt, Lynn Van Der and Verbeken, Brecht and Noever, David and Kopylov, Alexei and Myklebust, Benjamin and Li, Bikun and Schut, Lisa and Zheltonozhskii, Evgenii and Yuan, Qiaochu and Lim, Derek and Stanley, Richard and Yang, Tong and Maar, John and Wykowski, Julian and Oller, Mart{\'i} and Sahu, Anmol and Ardito, Cesare Giulio and Hu, Yuzheng and Kamdoum, Ariel Ghislain Kemogne and Jin, Alvin and Vilchis, Tobias Garcia and Zu, Yuexuan and Lackner, Martin and Koppel, James and Sun, Gongbo and Antonenko, Daniil S. and Chern, Steffi and Zhao, Bingchen and Arsene, Pierrot and Cavanagh, Joseph M. and Li, Daofeng and Shen, Jiawei and Crisostomi, Donato and Zhang, Wenjin and Dehghan, Ali and Ivanov, Sergey and Perrella, David and Kaparov, Nurdin and Zang, Allen and Sucholutsky, Ilia and Kharlamova, Arina and Orel, Daniil and Poritski, Vladislav and {Ben-David}, Shalev and Berger, Zachary and Whitfill, Parker and Foster, Michael and Munro, Daniel and Ho, Linh and Sivarajan, Shankar and Hava, Dan Bar and Kuchkin, Aleksey and Holmes, David and {Rodriguez-Romero}, Alexandra and Sommerhage, Frank and Zhang, Anji and Moat, Richard and Schneider, Keith and Kazibwe, Zakayo and Clarke, Don and Kim, Dae Hyun and Dias, Felipe Meneguitti and Fish, Sara and Elser, Veit and Kreiman, Tobias and Vilchis, Victor Efren Guadarrama and Klose, Immo and Anantheswaran, Ujjwala and Zweiger, Adam and Rawal, Kaivalya and Li, Jeffery and Nguyen, Jeremy and Daans, Nicolas and Heidinger, Haline and Radionov, Maksim and Rozho{\v n}, V{\'a}clav and Ginis, Vincent and Stump, Christian and Cohen, Niv and Po{\'s}wiata, Rafa{\l} and Tkadlec, Josef and Goldfarb, Alan and Wang, Chenguang and Padlewski, Piotr and Barzowski, Stanislaw and Montgomery, Kyle and Stendall, Ryan and {Tucker-Foltz}, Jamie and Stade, Jack and Rogers, T. Ryan and Goertzen, Tom and Grabb, Declan and Shukla, Abhishek and Givr{\'e}, Alan and Ambay, John Arnold and Sen, Archan and Aziz, Muhammad Fayez and Inlow, Mark H. and He, Hao and Zhang, Ling and Kaddar, Younesse and {\"A}ngquist, Ivar and Chen, Yanxu and Wang, Harrison K. and Ramakrishnan, Kalyan and Thornley, Elliott and Terpin, Antonio and Schoelkopf, Hailey and Zheng, Eric and Carmi, Avishy and Brown, Ethan D. L. and Zhu, Kelin and Bartolo, Max and Wheeler, Richard and Stehberger, Martin and Bradshaw, Peter and Heimonen, J. P. and Sridhar, Kaustubh and Akov, Ido and Sandlin, Jennifer and Makarychev, Yury and Tam, Joanna and Hoang, Hieu and Cunningham, David M. and Goryachev, Vladimir and Patramanis, Demosthenes and Krause, Michael and Redenti, Andrew and Aldous, David and Lai, Jesyin and Coleman, Shannon and Xu, Jiangnan and Lee, Sangwon and Magoulas, Ilias and Zhao, Sandy and Tang, Ning and Cohen, Michael K. and Paradise, Orr and Kirchner, Jan Hendrik and Ovchynnikov, Maksym and Matos, Jason O. and Shenoy, Adithya and Wang, Michael and Nie, Yuzhou and {Sztyber-Betley}, Anna and Faraboschi, Paolo and Riblet, Robin and Crozier, Jonathan and Halasyamani, Shiv and Verma, Shreyas and Joshi, Prashant and Meril, Eli and Ma, Ziqiao and Andr{\'e}oletti, J{\'e}r{\'e}my and Singhal, Raghav and Platnick, Jacob and Nevirkovets, Volodymyr and Basler, Luke and Ivanov, Alexander and Khoury, Seri and Gustafsson, Nils and Piccardo, Marco and Mostaghimi, Hamid and Chen, Qijia and Singh, Virendra and Kh{\'a}nh, Tran Quoc and Rosu, Paul and Szlyk, Hannah and Brown, Zachary and Narayan, Himanshu and Menezes, Aline and Roberts, Jonathan and Alley, William and Sun, Kunyang and Patel, Arkil and Lamparth, Max and Reuel, Anka and Xin, Linwei and Xu, Hanmeng and Loader, Jacob and Martin, Freddie and Wang, Zixuan and Achilleos, Andrea and Preu, Thomas and Korbak, Tomek and Bosio, Ida and Kazemi, Fereshteh and Chen, Ziye and B{\'a}lint, Bir{\'o} and Lo, Eve J. Y. and Wang, Jiaqi and Nunes, Maria In{\^e}s S. and Milbauer, Jeremiah and Bari, M. Saiful and Wang, Zihao and Ansarinejad, Behzad and Sun, Yewen and Durand, Stephane and Elgnainy, Hossam and Douville, Guillaume and Tordera, Daniel and Balabanian, George and Wolff, Hew and Kvistad, Lynna and Milliron, Hsiaoyun and Sakor, Ahmad and Eron, Murat and O, Andrew Favre D. and Shah, Shailesh and Zhou, Xiaoxiang and Kamalov, Firuz and Abdoli, Sherwin and Santens, Tim and Barkan, Shaul and Tee, Allison and Zhang, Robin and Tomasiello, Alessandro and Luca, G. Bruno De and Looi, Shi-Zhuo and Le, Vinh-Kha and Kolt, Noam and Pan, Jiayi and Rodman, Emma and Drori, Jacob and Fossum, Carl J. and Muennighoff, Niklas and Jagota, Milind and Pradeep, Ronak and Fan, Honglu and Eicher, Jonathan and Chen, Michael and Thaman, Kushal and Merrill, William and Firsching, Moritz and Harris, Carter and Ciob{\^a}c{\u a}, Stefan and Gross, Jason and Pandey, Rohan and Gusev, Ilya and Jones, Adam and Agnihotri, Shashank and Zhelnov, Pavel and Mofayezi, Mohammadreza and Piperski, Alexander and Zhang, David K. and Dobarskyi, Kostiantyn and Leventov, Roman and Soroko, Ignat and Duersch, Joshua and Taamazyan, Vage and Ho, Andrew and Ma, Wenjie and Held, William and Xian, Ruicheng and Zebaze, Armel Randy and Mohamed, Mohanad and Leser, Julian Noah and Yuan, Michelle X. and Yacar, Laila and Lengler, Johannes and Olszewska, Katarzyna and Fratta, Claudio Di and Oliveira, Edson and Jackson, Joseph W. and Zou, Andy and Chidambaram, Muthu and Manik, Timothy and Haffenden, Hector and Stander, Dashiell and Dasouqi, Ali and Shen, Alexander and Golshani, Bita and Stap, David and Kretov, Egor and Uzhou, Mikalai and Zhidkovskaya, Alina Borisovna and Winter, Nick and Rodriguez, Miguel Orbegozo and Lauff, Robert and Wehr, Dustin and Tang, Colin and Hossain, Zaki and Phillips, Shaun and Samuele, Fortuna and Ekstr{\"o}m, Fredrik and Hammon, Angela and Patel, Oam and Farhidi, Faraz and Medley, George and Mohammadzadeh, Forough and Pe{\~n}aflor, Madellene and Kassahun, Haile and Friedrich, Alena and Perez, Rayner Hernandez and Pyda, Daniel and Sakal, Taom and Dhamane, Omkar and Mirabadi, Ali Khajegili and Hallman, Eric and Okutsu, Kenchi and Battaglia, Mike and Maghsoudimehrabani, Mohammad and Amit, Alon and Hulbert, Dave and Pereira, Roberto and Weber, Simon and Handoko and Peristyy, Anton and Malina, Stephen and Mehkary, Mustafa and Aly, Rami and Reidegeld, Frank and Dick, Anna-Katharina and Friday, Cary and Singh, Mukhwinder and Shapourian, Hassan and Kim, Wanyoung and Costa, Mariana and Gurdogan, Hubeyb and Kumar, Harsh and Ceconello, Chiara and Zhuang, Chao and Park, Haon and Carroll, Micah and Tawfeek, Andrew R. and Steinerberger, Stefan and Aggarwal, Daattavya and Kirchhof, Michael and Dai, Linjie and Kim, Evan and Ferret, Johan and Shah, Jainam and Wang, Yuzhou and Yan, Minghao and Burdzy, Krzysztof and Zhang, Lixin and Franca, Antonio and Pham, Diana T. and Loh, Kang Yong and Robinson, Joshua and Jackson, Abram and Giordano, Paolo and Petersen, Philipp and Cosma, Adrian and Colino, Jesus and White, Colin and Votava, Jacob and Vinnikov, Vladimir and Delaney, Ethan and Spelda, Petr and Stritecky, Vit and Shahid, Syed M. and Mourrat, Jean-Christophe and Vetoshkin, Lavr and Sponselee, Koen and Bacho, Renas and Yong, Zheng-Xin and de la Rosa, Florencia and Cho, Nathan and Li, Xiuyu and Malod, Guillaume and Weller, Orion and Albani, Guglielmo and Lang, Leon and Laurendeau, Julien and Kazakov, Dmitry and Adesanya, Fatimah and Portier, Julien and Hollom, Lawrence and Souza, Victor and Zhou, Yuchen Anna and Degorre, Julien and Yal{\i}n, Yi{\u g}it and Obikoya, Gbenga Daniel and Rai and Bigi, Filippo and Bosc{\'a}, M. C. and Shumar, Oleg and Bacho, Kaniuar and Recchia, Gabriel and Popescu, Mara and Shulga, Nikita and Tanwie, Ngefor Mildred and Lux, Thomas C. H. and Rank, Ben and Ni, Colin and Brooks, Matthew and Yakimchyk, Alesia and Huanxu and Liu and Cavalleri, Stefano and H{\"a}ggstr{\"o}m, Olle and Verkama, Emil and Newbould, Joshua and Gundlach, Hans and {Brito-Santana}, Leonor and Amaro, Brian and Vajipey, Vivek and Grover, Rynaa and Wang, Ting and Kratish, Yosi and Li, Wen-Ding and Gopi, Sivakanth and Caciolai, Andrea and de Witt, Christian Schroeder and {Hern{\'a}ndez-C{\'a}mara}, Pablo and Rodol{\`a}, Emanuele and Robins, Jules and Williamson, Dominic and Cheng, Vincent and Raynor, Brad and Qi, Hao and Segev, Ben and Fan, Jingxuan and Martinson, Sarah and Wang, Erik Y. and Hausknecht, Kaylie and Brenner, Michael P. and Mao, Mao and Demian, Christoph and Kassani, Peyman and Zhang, Xinyu and Avagian, David and Scipio, Eshawn Jessica and Ragoler, Alon and Tan, Justin and Sims, Blake and Plecnik, Rebeka and Kirtland, Aaron and Bodur, Omer Faruk and Shinde, D. P. and Labrador, Yan Carlos Leyva and Adoul, Zahra and Zekry, Mohamed and Karakoc, Ali and Santos, Tania C. B. and Shamseldeen, Samir and Karim, Loukmane and Liakhovitskaia, Anna and Resman, Nate and Farina, Nicholas and Gonzalez, Juan Carlos and Maayan, Gabe and Anderson, Earth and Pena, Rodrigo De Oliveira and Kelley, Elizabeth and Mariji, Hodjat and Pouriamanesh, Rasoul and Wu, Wentao and Finocchio, Ross and Alarab, Ismail and Cole, Joshua and Ferreira, Danyelle and Johnson, Bryan and Safdari, Mohammad and Dai, Liangti and Arthornthurasuk, Siriphan and McAlister, Isaac C. and Moyano, Alejandro Jos{\'e} and Pronin, Alexey and Fan, Jing and {Ramirez-Trinidad}, Angel and Malysheva, Yana and Pottmaier, Daphiny and Taheri, Omid and Stepanic, Stanley and Perry, Samuel and Askew, Luke and Rodr{\'i}guez, Ra{\'u}l Adri{\'a}n Huerta and Minissi, Ali M. R. and Lorena, Ricardo and Iyer, Krishnamurthy and Fasiludeen, Arshad Anil and Clark, Ronald and Ducey, Josh and Piza, Matheus and Somrak, Maja and Vergo, Eric and Qin, Juehang and Borb{\'a}s, Benj{\'a}min and Chu, Eric and Lindsey, Jack and Jallon, Antoine and McInnis, I. M. J. and Chen, Evan and Semler, Avi and Gloor, Luk and Shah, Tej and Carauleanu, Marc and Lauer, Pascal and Huy, Tran {\DJ}uc and Shahrtash, Hossein and Duc, Emilien and Lewark, Lukas and Brown, Assaf and Albanie, Samuel and Weber, Brian and Vaz, Warren S. and Clavier, Pierre and Fan, Yiyang and e Silva, Gabriel Poesia Reis and Long and Lian and Abramovitch, Marcus and Jiang, Xi and Mendoza, Sandra and Islam, Murat and Gonzalez, Juan and Mavroudis, Vasilios and Xu, Justin and Kumar, Pawan and Goswami, Laxman Prasad and Bugas, Daniel and Heydari, Nasser and Jeanplong, Ferenc and Jansen, Thorben and Pinto, Antonella and Apronti, Archimedes and Galal, Abdallah and {Ze-An}, Ng and Singh, Ankit and Jiang, Tong and Xavier, Joan of Arc and Agarwal, Kanu Priya and Berkani, Mohammed and Zhang, Gang and Du, Zhehang and Junior, Benedito Alves de Oliveira and Malishev, Dmitry and Remy, Nicolas and Hartman, Taylor D. and Tarver, Tim and Mensah, Stephen and Loume, Gautier Abou and Morak, Wiktor and Habibi, Farzad and Hoback, Sarah and Cai, Will and Gimenez, Javier and Montecillo, Roselynn Grace and {\L}ucki, Jakub and Campbell, Russell and Sharma, Asankhaya and Meer, Khalida and Gul, Shreen and Gonzalez, Daniel Espinosa and Alapont, Xavier and Hoover, Alex and Chhablani, Gunjan and Vargus, Freddie and Agarwal, Arunim and Jiang, Yibo and Patil, Deepakkumar and Outevsky, David and Scaria, Kevin Joseph and Maheshwari, Rajat and Dendane, Abdelkader and Shukla, Priti and Cartwright, Ashley and Bogdanov, Sergei and M{\"u}ndler, Niels and M{\"o}ller, S{\"o}ren and Arnaboldi, Luca and Thaman, Kunvar and Siddiqi, Muhammad Rehan and Saxena, Prajvi and Gupta, Himanshu and Fruhauff, Tony and Sherman, Glen and Vincze, M{\'a}ty{\'a}s and Usawasutsakorn, Siranut and Ler, Dylan and Radhakrishnan, Anil and Enyekwe, Innocent and Salauddin, Sk Md and Muzhen, Jiang and Maksapetyan, Aleksandr and Rossbach, Vivien and Harjadi, Chris and Bahaloohoreh, Mohsen and Sparrow, Claire and Sidhu, Jasdeep and Ali, Sam and Bian, Song and Lai, John and Singer, Eric and Uro, Justine Leon and Bateman, Greg and Sayed, Mohamed and Menshawy, Ahmed and Duclosel, Darling and Bezzi, Dario and Jain, Yashaswini and Aaron, Ashley and Tiryakioglu, Murat and Siddh, Sheeshram and Krenek, Keith and Shah, Imad Ali and Jin, Jun and Creighton, Scott and Peskoff, Denis and {EL-Wasif}, Zienab and V, Ragavendran P. and Richmond, Michael and McGowan, Joseph and Patwardhan, Tejal and Sun, Hao-Yu and Sun, Ting and Zubi{\'c}, Nikola and Sala, Samuele and Ebert, Stephen and Kaddour, Jean and Schottdorf, Manuel and Wang, Dianzhuo and Petruzella, Gerol and Meiburg, Alex and Medved, Tilen and ElSheikh, Ali and Hebbar, S. Ashwin and Vaquero, Lorenzo and Yang, Xianjun and Poulos, Jason and Zouhar, Vil{\'e}m and Bogdanik, Sergey and Zhang, Mingfang and {Sanz-Ros}, Jorge and Anugraha, David and Dai, Yinwei and Nhu, Anh N. and Wang, Xue and Demircali, Ali Anil and Jia, Zhibai and Zhou, Yuyin and Wu, Juncheng and He, Mike and Chandok, Nitin and Sinha, Aarush and Luo, Gaoxiang and Le, Long and Noy{\'e}, Micka{\"e}l and Pere{\l}kiewicz, Micha{\l} and Pantidis, Ioannis and Qi, Tianbo and Purohit, Soham Sachin and Parcalabescu, Letitia and Nguyen, Thai-Hoa and Winata, Genta Indra and Ponti, Edoardo M. and Li, Hanchen and Dhole, Kaustubh and Park, Jongee and Abbondanza, Dario and Wang, Yuanli and Nayak, Anupam and Caetano, Diogo M. and Wong, Antonio A. W. L. and del {Rio-Chanona}, Maria and Kondor, D{\'a}niel and Francois, Pieter and Chalstrey, Ed and Zsambok, Jakob and Hoyer, Dan and Reddish, Jenny and Hauser, Jakob and {Rodrigo-Gin{\'e}s}, Francisco-Javier and Datta, Suchandra and Shepherd, Maxwell and Kamphuis, Thom and Zhang, Qizheng and Kim, Hyunjun and Sun, Ruiji and Yao, Jianzhu and Dernoncourt, Franck and Krishna, Satyapriya and Rismanchian, Sina and Pu, Bonan and Pinto, Francesco and Wang, Yingheng and Shridhar, Kumar and Overholt, Kalon J. and Briia, Glib and Nguyen, Hieu and David and Bartomeu, Soler and Pang, Tony CY and Wecker, Adam and Xiong, Yifan and Li, Fanfei and Huber, Lukas S. and Jaeger, Joshua and Maddalena, Romano De and L{\`u}, Xing Han and Zhang, Yuhui and Beger, Claas and Kon, Patrick Tser Jern and Li, Sean and Sanker, Vivek and Yin, Ming and Liang, Yihao and Zhang, Xinlu and Agrawal, Ankit and Yifei, Li S. and Zhang, Zechen and Cai, Mu and Sonmez, Yasin and Cozianu, Costin and Li, Changhao and Slen, Alex and Yu, Shoubin and Park, Hyun Kyu and Sarti, Gabriele and Bria{\'n}ski, Marcin and Stolfo, Alessandro and Nguyen, Truong An and Zhang, Mike and Perlitz, Yotam and {Hernandez-Orallo}, Jose and Li, Runjia and Shabani, Amin and {Juefei-Xu}, Felix and Dhingra, Shikhar and Zohar, Orr and Nguyen, My Chiffon and Pondaven, Alexander and Yilmaz, Abdurrahim and Zhao, Xuandong and Jin, Chuanyang and Jiang, Muyan and Todoran, Stefan and Han, Xinyao and Kreuer, Jules and Rabern, Brian and Plassart, Anna and Maggetti, Martino and Yap, Luther and Geirhos, Robert and Kean, Jonathon and Wang, Dingsu and Mollaei, Sina and Sun, Chenkai and Yin, Yifan and Wang, Shiqi and Li, Rui and Chang, Yaowen and Wei, Anjiang and Bizeul, Alice and Wang, Xiaohan and Arrais, Alexandre Oliveira and Mukherjee, Kushin and {Chamorro-Padial}, Jorge and Liu, Jiachen and Qu, Xingyu and Guan, Junyi and Bouyamourn, Adam and Wu, Shuyu and Plomecka, Martyna and Chen, Junda and Tang, Mengze and Deng, Jiaqi and Subramanian, Shreyas and Xi, Haocheng and Chen, Haoxuan and Zhang, Weizhi and Ren, Yinuo and Tu, Haoqin and Kim, Sejong and Chen, Yushun and Marjanovi{\'c}, Sara Vera and Ha, Junwoo and Luczyna, Grzegorz and Ma, Jeff J. and Shen, Zewen and Song, Dawn and Zhang, Cedegao E. and Wang, Zhun and Gendron, Ga{\"e}l and Xiao, Yunze and Smucker, Leo and Weng, Erica and Lee, Kwok Hao and Ye, Zhe and Ermon, Stefano and {Lopez-Miguel}, Ignacio D. and Knights, Theo and Gitter, Anthony and Park, Namkyu and Wei, Boyi and Chen, Hongzheng and Pai, Kunal and Elkhanany, Ahmed and Lin, Han and Siedler, Philipp D. and Fang, Jichao and Mishra, Ritwik and {Zsolnai-Feh{\'e}r}, K{\'a}roly and Jiang, Xilin and Khan, Shadab and Yuan, Jun and Jain, Rishab Kumar and Lin, Xi and Peterson, Mike and Wang, Zhe and Malusare, Aditya and Tang, Maosen and Gupta, Isha and Fosin, Ivan and Kang, Timothy and Dworakowska, Barbara and Matsumoto, Kazuki and Zheng, Guangyao and Sewuster, Gerben and Villanueva, Jorge Pretel and Rannev, Ivan and Chernyavsky, Igor and Chen, Jiale and Banik, Deepayan and Racz, Ben and Dong, Wenchao and Wang, Jianxin and Bashmal, Laila and Gon{\c c}alves, Duarte V. and Hu, Wei and Bar, Kaushik and Bohdal, Ondrej and Patlan, Atharv Singh and Dhuliawala, Shehzaad and Geirhos, Caroline and Wist, Julien and Kansal, Yuval and Chen, Bingsen and Tire, Kutay and Y{\"u}cel, Atak Talay and Christof, Brandon and Singla, Veerupaksh and Song, Zijian and Chen, Sanxing and Ge, Jiaxin and Ponkshe, Kaustubh and Park, Isaac and Shi, Tianneng and Ma, Martin Q. and Mak, Joshua and Lai, Sherwin and Moulin, Antoine and Cheng, Zhuo and Zhu, Zhanda and Zhang, Ziyi and Patil, Vaidehi and Jha, Ketan and Men, Qiutong and Wu, Jiaxuan and Zhang, Tianchi and Vieira, Bruno Hebling and Aji, Alham Fikri and Chung, Jae-Won and Mahfoud, Mohammed and Hoang, Ha Thi and Sperzel, Marc and Hao, Wei and Meding, Kristof and Xu, Sihan and Kostakos, Vassilis and Manini, Davide and Liu, Yueying and Toukmaji, Christopher and Paek, Jay and Yu, Eunmi and Demircali, Arif Engin and Sun, Zhiyi and Dewerpe, Ivan and Qin, Hongsen and Pflugfelder, Roman and Bailey, James and Morris, Johnathan and Heilala, Ville and Rosset, Sybille and Yu, Zishun and Chen, Peter E. and Yeo, Woongyeong and Jain, Eeshaan and Yang, Ryan and Chigurupati, Sreekar and Chernyavsky, Julia and Reddy, Sai Prajwal and Venugopalan, Subhashini and Batra, Hunar and Park, Core Francisco and Tran, Hieu and Maximiano, Guilherme and Zhang, Genghan and Liang, Yizhuo and Shiyu, Hu and Xu, Rongwu and Pan, Rui and Suresh, Siddharth and Liu, Ziqi and Gulati, Samaksh and Zhang, Songyang and Turchin, Peter and Bartlett, Christopher W. and Scotese, Christopher R. and Cao, Phuong M. and Nattanmai, Aakaash and McKellips, Gordon and Cheraku, Anish and Suhail, Asim and Luo, Ethan and Deng, Marvin and Luo, Jason and Zhang, Ashley and Jindel, Kavin and Paek, Jay and Halevy, Kasper and Baranov, Allen and Liu, Michael and Avadhanam, Advaith and Zhang, David and Cheng, Vincent and Ma, Brad and Fu, Evan and Do, Liam and Lass, Joshua and Yang, Hubert and Sunkari, Surya and Bharath, Vishruth and Ai, Violet and Leung, James and Agrawal, Rishit and Zhou, Alan and Chen, Kevin and Kalpathi, Tejas and Xu, Ziqi and Wang, Gavin and Xiao, Tyler and Maung, Erik and Lee, Sam and Yang, Ryan and Yue, Roy and Zhao, Ben and Yoon, Julia and Sun, Sunny and Singh, Aryan and Luo, Ethan and Peng, Clark and Osbey, Tyler and Wang, Taozhi and Echeazu, Daryl and Yang, Hubert and Wu, Timothy and Patel, Spandan and Kulkarni, Vidhi and Sundarapandiyan, Vijaykaarti and Zhang, Ashley and Le, Andrew and Nasim, Zafir and Yalam, Srikar and Kasamsetty, Ritesh and Samal, Soham and Yang, Hubert and Sun, David and Shah, Nihar and Saha, Abhijeet and Zhang, Alex and Nguyen, Leon and Nagumalli, Laasya and Wang, Kaixin and Zhou, Alan and Wu, Aidan and Luo, Jason and Telluri, Anwith and Yue, Summer and Wang, Alexandr and Hendrycks, Dan},
  year = {2025},
  month = apr,
  number = {arXiv:2501.14249},
  eprint = {2501.14249},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.14249},
  urldate = {2025-06-02},
  abstract = {Benchmarks are important tools for tracking the rapid advancements in large language model (LLM) capabilities. However, benchmarks are not keeping pace in difficulty: LLMs now achieve over 90{\textbackslash}\% accuracy on popular benchmarks like MMLU, limiting informed measurement of state-of-the-art LLM capabilities. In response, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at the frontier of human knowledge, designed to be the final closed-ended academic benchmark of its kind with broad subject coverage. HLE consists of 2,500 questions across dozens of subjects, including mathematics, humanities, and the natural sciences. HLE is developed globally by subject-matter experts and consists of multiple-choice and short-answer questions suitable for automated grading. Each question has a known solution that is unambiguous and easily verifiable, but cannot be quickly answered via internet retrieval. State-of-the-art LLMs demonstrate low accuracy and calibration on HLE, highlighting a significant gap between current LLM capabilities and the expert human frontier on closed-ended academic questions. To inform research and policymaking upon a clear understanding of model capabilities, we publicly release HLE at https://lastexam.ai.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\Dell\Zotero\storage\MLFYKR6L\Phan et al. - 2025 - Humanity's Last Exam.pdf}
}

@book{pigliucciHandbookNewStoics2019,
  title = {A Handbook for New {{Stoics}}: How to Thrive in a World out of Your Control : 52 Week-by-Week Lessons},
  shorttitle = {A Handbook for New {{Stoics}}},
  author = {Pigliucci, Massimo and Lopez, Gregory},
  year = {2019},
  publisher = {The Experiment},
  address = {New York},
  abstract = {"Stress often comes from situations that are beyond our control. But we can control our response to these everyday tensions through the wisdom and practice of Stoicism, an ancient pragmatic philosophy that teaches us to step back, gain perspective, and act with intention.The authors provide 52 week-by-week lessons to help us apply timeless Stoic teachings to modern life." -- (Source of summary not specified)},
  isbn = {978-1-61519-533-6},
  langid = {english},
  annotation = {OCLC: 1041893346},
  file = {C:\Users\Dell\Zotero\storage\3Y9J8B6X\Pigliucci and Lopez - 2019 - A handbook for new Stoics how to thrive in a world out of your control  52 week-by-week lessons.pdf}
}

@book{publishingSummaryFourAgreements2018,
  title = {Summary of the {{Four Agreements}}},
  author = {Publishing, Readtrepreneur},
  year = {2018},
  month = jun,
  publisher = {Readtrepreneur Publishing},
  abstract = {{$<$}p{$><$}strong{$>$}The Four Agreements: A Practical Guide to Personal{$<$}br /{$>$}Freedom by Miguel Ruiz {\textbar} Book Summary {\textbar} Readtrepreneur{$<$}/strong{$><$}br /{$>$}(Disclaimer: This is NOT the original book, but an unofficial summary.){$<$}/p{$><$}p{$><$}br /{$>$}Have you ever felt that every element in your life is not fitting in just the way you{$<$}br /{$>$}pictured? You are not alone and it's never late to fix that issue.{$<$}br /{$>$}In The Four Agreements Don Miguel Ruiz tell us the core of self-limiting beliefs that prevent us{$<$}br /{$>$}from reaching a stable status of joy and make us suffer. These are common evils that we, as{$<$}br /{$>$}human possess and must get rid of. With Ruiz's guidance, you will can do so.{$<$}br /{$>$}(Note: This summary is wholly written and published by Readtrepreneur.\&nbsp;It is not affiliated{$<$}br /{$>$}with the original author in any way){$<$}/p{$><$}p{$><$}strong{$>$}"Whatever happens around you, don't take it personally\&hellip;{$<$}br /{$>$}Nothing other people do is because of you. It is because of{$<$}br /{$>$}themselves." \&ndash; Miguel...},
  langid = {english},
  keywords = {companion book master dream life power now guide,four agreements practical guide personal freedom,new google e non fiction guide grit of for kids,spiritual englightenment untethered soul journey,summary 2019 2020 top book books paperback online,toltec wisdom book mastery love art relationship,voice knowledge don miguel ruiz fifth agreement},
  annotation = {Item ID: \_:n0},
  file = {C:\Users\Dell\Zotero\storage\CKF7H5MX\Publishing - 2018 - Summary of the Four Agreements.epub}
}

@book{raghavanBillionairesApprenticeRise2013,
  title = {The {{Billionaire}}'s {{Apprentice}}: {{The Rise}} of the {{Indian-American Elite}} and the {{Fall}} of the {{Galleon Hedge Fund}}},
  author = {Raghavan, Anita},
  year = {2013},
  month = jun,
  publisher = {Business Plus},
  abstract = {{$<$}div{$><$}p{$>$}Just as WASPs, Irish-Catholics and Our Crowd Jews once made the ascent from immigrants to powerbrokers, it is now the Indian-American's turn. Citigroup, PepsiCo and Mastercard are just a handful of the Fortune 500 companies led by a group known as the "Twice Blessed." Yet little is known about how these Indian emigres (and children of emigres) rose through the ranks. Until now...{$<$}br{$><$}/p{$><$}p{$>$}The collapse of the Galleon Group--a hedge fund that managed more than \$7 billion in assets--from criminal charges of insider trading was a sensational case that pitted prosecutor Preet Bharara, himself the son of Indian immigrants, against the best and brightest of the South Asian business community. At the center of the case was self-described King of Kings, Galleon's founder Raj Rajaratnam, a Sri-Lankan-born, Wharton-educated billionaire. But the most shocking allegation was that the {\'e}minence grise of Indian business, Rajat Gupta, was Rajaratnam's accomplice and mole. If not for Gupta's nose-to-the-grindstone rise to head up McKinsey \&amp; Co and a position on the Goldman Sachs board, men like Rajaratnam would have never made it to the top of America's moneyed elite.{$<$}br{$><$}/p{$><$}p{$>$}Author Anita Raghavan criss-crosses the globe from Wall Street boardrooms to Delhi's Indian Institute of Technology as she uncovers the secrets of this subculture--an incredible tale of triumph, temptation and tragedy.{$<$}br{$><$}/p{$><$}h3{$>$}Review{$<$}/h3{$><$}p{$>$}I've always wondered how Indian-Americans came out of nowhere to become a force in the business establishment, The Billionaire's Apprentice explains that meteoric rise, but it is also a page-turning cops and robbers story set against the backdrops of Silicon Valley and Wall Street.{$<$}br{$><$}/p{$><$}p{$>$}--Adam Lashinsky, bestselling author of Inside Apple: How America's Most Admired-and Secretive-Company Really Works{$<$}br{$><$}/p{$><$}p{$>$}Thanks to author Anita Raghavan's intrepid reporting, The Billionaire's Apprentice combines the drama of the federal government unraveling an insider trading ring with the historical sweep of immigrants rising from nothing to the corridors of corporate power.{$<$}br{$><$}/p{$><$}p{$>$}-- Bethany McLean, co-author of the bestsellers The Smartest Guys In the Room and All the Devils Are Here{$<$}br{$><$}/p{$><$}p{$>$}Anita Raghavan's journalistic and writing skill comes through on every page of THE BILLIONAIRE'S APPRENTICE. I couldn't put it down; it's a true story that reads like a thriller.{$<$}br{$><$}/p{$><$}p{$>$}--William D. Cohan, bestselling author of The Last Tycoons, House of Cards, and Money and Power{$<$}br{$><$}/p{$><$}p{$>$}THE BILLIONAIRE'S APPRENTICE is that rare work of nonfiction that follows an ambitious hero as he climbs to the pinnacle of power inside the top boardrooms of corporate America, gets seduced, and falls in a spectacular insider trading scandal. This is a modern-day Greek tragedy that plays out among the upper echelons of Wall Street, Silicon Valley, and the global business elite. Doggedly reported and utterly compelling.{$<$}br{$><$}/p{$><$}p{$>$}--Bryan Burrough, bestselling author of Barbarians at the Gateand The Big Rich{$<$}br{$><$}/p{$><$}p{$>$}Raghavan excels with her account.... She provides an insightful account of South Asian immigration to the U.S. since the 1960s and shows how relations established in India's elite education system provided some of the ties that bound the conspirators together.{$<$}br{$><$}/p{$><$}p{$>$}-- Kirkus{$<$}br{$><$}/p{$><$}p{$>$}Through meticulous research, copious history, vivid characters, and entertaining prose, Raghavan weaves together many different worlds, eras, and personality types to deliver a compelling view of the multi-cultural politics of today's Wall Street.{$<$}br{$><$}/p{$><$}p{$>$}-- Huffington Post{$<$}br{$><$}/p{$><$}p{$>$}Riveting.{$<$}br{$><$}/p{$><$}p{$>$}--Bloomberg {$<$}br{$><$}/p{$><$}p{$>$}The best form of journalism, an early draft of history.{$<$}br{$><$}/p{$><$}p{$>$}-- The New York Times Book Review{$<$}br{$><$}/p{$><$}p{$>$}Anita Raghavan's 'The Billionaire's Apprentice,' [is] a riveting account of the takedown of Raj Rajaratnam...[Ms. Raghavan] has written a briskly paced account full of fascinating detail...this book deserves to be on the shelf of anyone lusting after a little Wall Street schadenfreude this summer.{$<$}br{$><$}/p{$><$}p{$>$}-- WSJ{$<$}br{$><$}/p{$><$}p{$>$}[a] deeply researched, fascinating and well-written book.{$<$}br{$><$}/p{$><$}p{$>$}-- Financial Times{$<$}br{$><$}/p{$><$}h3{$>$}About the Author{$<$}/h3{$><$}p{$>$}Anita Raghavan was born in Malaysia but came to the United States in 1970. A graduate of the University of Pennsylvania, she spent eighteen years at the Wall Street Journal where she won the Overseas Press Club award for her coverage of the mergers and acquisition boom in Europe, and the New York Press Club award for her reporting on the the near death of the hedge fund Long-Term Capital. In 2008, she became the London Bureau Chief for Forbes. Currently she is a contributor to New York Times Dealbook and Forbes. {$<$}br{$><$}/p{$><$}/div{$>$}},
  isbn = {978-1-4555-0403-9},
  langid = {english},
  keywords = {Business & Economics,Finance},
  annotation = {Item ID: \_:n0},
  file = {C:\Users\Dell\Zotero\storage\BX5PHPRT\Raghavan - 2013 - The Billionaire's Apprentice The Rise of the Indian-American Elite and the Fall of the Galleon Hedg.epub}
}

@misc{raiPracticalReviewMechanistic2025,
  title = {A {{Practical Review}} of {{Mechanistic Interpretability}} for {{Transformer-Based Language Models}}},
  author = {Rai, Daking and Zhou, Yilun and Feng, Shi and Saparov, Abulhair and Yao, Ziyu},
  year = {2025},
  month = mar,
  number = {arXiv:2407.02646},
  eprint = {2407.02646},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.02646},
  urldate = {2025-03-27},
  abstract = {Mechanistic interpretability (MI) is an emerging sub-field of interpretability that seeks to understand a neural network model by reverse-engineering its internal computations. Recently, MI has garnered significant attention for interpreting transformer-based language models (LMs), resulting in many novel insights yet introducing new challenges. However, there has not been work that comprehensively reviews these insights and challenges, particularly as a guide for newcomers to this field. To fill this gap, we provide a comprehensive survey from a task-centric perspective, organizing the taxonomy of MI research around specific research questions or tasks. We outline the fundamental objects of study in MI, along with the techniques, evaluation methods, and key findings for each task in the taxonomy. In particular, we present a task-centric taxonomy as a roadmap for beginners to navigate the field by helping them quickly identify impactful problems in which they are most interested and leverage MI for their benefit. Finally, we discuss the current gaps in the field and suggest potential future directions for MI research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C:\Users\Dell\Zotero\storage\IY29QBZJ\Rai et al. - 2025 - A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models.pdf}
}

@article{raschkaBuildLargeLanguage,
  title = {Build a {{Large Language Model}} ({{From Scratch}})},
  author = {Raschka, Sebastian},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\M9NTRG9Z\Raschka - Build a Large Language Model (From Scratch).pdf}
}

@book{raschkaMachineLearningAI2024,
  title = {Machine {{Learning Q}} and {{AI}}: 30 {{Essential Questions}} and {{Answers}} on {{Machine Learning}} and {{AI}}},
  shorttitle = {Machine {{Learning Q}} and {{AI}}},
  author = {Raschka, Sebastian},
  year = {2024},
  edition = {1st ed},
  publisher = {No Starch Press},
  address = {New York},
  abstract = {"An advanced exploration of machine learning and AI, with each chapter asking and answering a question from the field. Divided into five sections: deep learning and neural networks; computer vision; natural language processing; production and deployment; and predictive performance and model evaluation"--},
  isbn = {978-1-7185-0376-2 978-1-7185-0377-9},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\LK2NR5Y8\Machine Learning Q and AI_ 30 Essential Questions and -- Sebastian Raschka -- 1 _ converted, 2024 -- No Starch Press, San Francisco -- 9781718503762 -- d6cad11d227d89b85744435fde445441 -- Anna’s Archive.pdf}
}

@misc{raschkaUnderstandingCodingSelfAttention0000,
  title = {Understanding and {{Coding}} the {{Self-Attention Mechanism}} of {{Large Language Models From Scratch}}},
  author = {Raschka, Sebastian},
  year = {08:00:00 +0000},
  journal = {Sebastian Raschka, PhD},
  urldate = {2025-03-26},
  abstract = {In this article, we are going to understand how self-attention works from scratch. This means we will code it ourselves one step at a time. Since its introdu...},
  howpublished = {https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\L9KD4Q8M\self-attention-from-scratch.html}
}

@book{ruizMasteryLovePractical2011,
  title = {The {{Mastery}} of {{Love}}: {{A Practical Guide}} to the {{Art}} of {{Relationship}} ({{A Toltec Wisdom Book}})},
  author = {Ruiz, Don Miguel},
  year = {2011},
  month = jul,
  publisher = {Amber-Allen Publishing},
  copyright = {Copyright \&\#x00A9; 1999 by Miguel Angel Ruiz, M.D.},
  langid = {english},
  annotation = {Item ID: \_:n0},
  file = {C:\Users\Dell\Zotero\storage\8YRCBPX8\Ruiz - 2011 - The Mastery of Love A Practical Guide to the Art of Relationship (A Toltec Wisdom Book).epub}
}

@article{seacordEffective2ndEditiona,
  title = {Effective {{C}}, 2nd {{Edition}}},
  author = {Seacord, Robert C},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\4WZRT9MG\Seacord - Effective C, 2nd Edition.pdf}
}

@article{sellarsLessonsStoicism2020,
  title = {Lessons in {{Stoicism}}},
  author = {Sellars, John},
  year = {2020},
  month = jan,
  journal = {urn:isbn:9780241382776},
  publisher = {Penguin Books Ltd},
  abstract = {{$<$}p{$>$}\emph{What aspects of your life do you really control? What do you do when you cannot guarantee that things will turn out in your favour? And what can Stoicism teach us about how to live together?}{$<$}/p{$><$}p{$>$}In the past few years, Stoicism has been making a comeback. From online groups to Stoicism badges and how-to guides, this philosophical movement is gaining more followers. But what exactly did the Stoics believe and how did they live their lives? In \emph{Lessons in Stoicism}, philosopher John Sellars weaves together the key ideas of the three great Roman Stoics - Seneca, Epictetus and Marcus Aurelius - with snapshots of their fascinating lives. {$<$}/p{$><$}p{$>$}In vivid prose, Sellars shows how the works of these three Stoics have inspired readers ever since, speaking as they do to some of the perennial issues that face anyone trying to navigate their way through life. Their works, fundamentally, are about how to live - how to understand one's place in the world, how to cope when things...},
  copyright = {All rights reserved.},
  isbn = {9780241382929},
  langid = {english},
  annotation = {Item ID: \_:n0},
  file = {C:\Users\Dell\Zotero\storage\FF48NBLQ\Sellars - 2020 - Lessons in Stoicism.epub}
}

@misc{shojaeeIllusionThinkingUnderstanding2025,
  title = {The {{Illusion}} of {{Thinking}}: {{Understanding}} the {{Strengths}} and {{Limitations}} of {{Reasoning Models}} via the {{Lens}} of {{Problem Complexity}}},
  shorttitle = {The {{Illusion}} of {{Thinking}}},
  author = {Shojaee, Parshin and Mirzadeh, Iman and Alizadeh, Keivan and Horton, Maxwell and Bengio, Samy and Farajtabar, Mehrdad},
  year = {2025},
  month = jun,
  number = {arXiv:2506.06941},
  eprint = {2506.06941},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.06941},
  urldate = {2025-06-13},
  abstract = {Recent generations of frontier language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood. Current evaluations primarily focus on established mathematical and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from data contamination and does not provide insights into the reasoning traces' structure and quality. In this work, we systematically investigate these gaps with the help of controllable puzzle environments that allow precise manipulation of compositional complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs ``think''. Through extensive experimentation across diverse puzzles, we show that frontier LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having an adequate token budget. By comparing LRMs with their standard LLM counterparts under equivalent inference compute, we identify three performance regimes: (1) lowcomplexity tasks where standard models surprisingly outperform LRMs, (2) medium-complexity tasks where additional thinking in LRMs demonstrates advantage, and (3) high-complexity tasks where both models experience complete collapse. We found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across puzzles. We also investigate the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models' computational behavior, shedding light on their strengths, limitations, and ultimately raising crucial questions about their true reasoning capabilities.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\Dell\Zotero\storage\NLPJQQCT\Shojaee et al. - 2025 - The Illusion of Thinking Understanding the Strengths and Limitations of Reasoning Models via the Le.pdf}
}

@book{spraulHowSoftwareWorks2015,
  title = {How Software Works: The Magic behind Encryption, {{CGI}}, Search Engines, and Other Everyday Technologies},
  shorttitle = {How Software Works},
  author = {Spraul, V. Anton},
  year = {2015},
  publisher = {No Starch Press},
  address = {San Francisco},
  abstract = {"A guide for non-technical readers that explores topics like data encryption; computer graphics creation; password protection; video compression; how data is found in huge databases; how programs can work together on the same problem without conflict; and how map software finds routes."--},
  isbn = {978-1-59327-666-9},
  langid = {english},
  lccn = {QA76.5 .S6663 2015},
  keywords = {Computer networks,Computer software,Electronic data processing,Popular works},
  file = {C:\Users\Dell\Zotero\storage\K3FZF6VG\Spraul - 2015 - How software works the magic behind encryption, CGI, search engines, and other everyday technologie.pdf}
}

@book{stallingsComputerOrganizationArchitecture2016,
  title = {Computer Organization and Architecture: Designing for Performance},
  shorttitle = {Computer Organization and Architecture},
  author = {Stallings, William},
  year = {2016},
  edition = {Tenth edition},
  publisher = {Pearson-Prentice Hall},
  address = {Boston},
  isbn = {978-0-13-410161-3},
  langid = {english},
  lccn = {QA76.9.C643 S73 2016},
  keywords = {Computer architecture,Computer organization},
  file = {C:\Users\Dell\Zotero\storage\UV2HLBW4\Stallings - 2016 - Computer organization and architecture designing for performance.pdf}
}

@book{steinhartSecretLifePrograms2019,
  title = {The Secret Life of Programs: Understand Computers--Craft Better Code},
  shorttitle = {The Secret Life of Programs},
  author = {Steinhart, Jon},
  year = {2019},
  publisher = {No Starch Press, Inc},
  address = {San Francisco},
  abstract = {"Presents essential information about how computers work, including many broad topics, such as how computers process languages, how programs manipulate data in memory, and how web browsers work"--},
  isbn = {978-1-59327-970-7},
  langid = {english},
  lccn = {QA76.6 .S735 2019},
  keywords = {Browsers (Computer programs),Computer programming,Programming languages (Electronic computers)},
  annotation = {OCLC: on1099540411},
  file = {C:\Users\Dell\Zotero\storage\JPHGDEJ7\Steinhart - 2019 - The secret life of programs understand computers--craft better code.pdf}
}

@book{stephen7HabitsHighly2009,
  title = {The 7 Habits of Highly Effective People},
  author = {Stephen, R. Covey},
  year = {2009},
  abstract = {When it was first published, in 1990, The 7 Habits of Highly Effective People was an almost instant bestseller--and became a permanent part of the cultural lexicon. With more than 15 million copies sold in over 33 languages and 75 countries since its first publication, this book continues to help millions of people become more effective--both in their personal and professional lives. What author Stephen R. Covey advocates is no less than a paradigm shift--a major change in how readers perceive the world. This encompasses time management, proactivity, positive thinking, spiritual life, communication, and more. With compelling anecdotes and penetrating insight on every page, Dr. Covey guides the way to a life lived with integrity, service, dignity, and success--both at home and at work. ABOUT THE AUTHORA well-known authority on leadership and family relations, Dr. Stephen R. Covey holds a Bachelor of Science from the University of Utah, an MBA ..},
  isbn = {978-0-7953-0921-2},
  langid = {english},
  annotation = {OCLC: 870686732},
  file = {C:\Users\Dell\Zotero\storage\X8JWVMZZ\Stephen - 2009 - The 7 habits of highly effective people.pdf}
}

@article{stroustrupWhyNotJust,
  title = {Why {{C}}++ Is Not Just an {{Object-Oriented Programming Language}}},
  author = {Stroustrup, Bjarne},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\BCKHDAFQ\Stroustrup - Why C++ is not just an Object-Oriented Programming Language.pdf}
}

@book{stutzLinuxCookbookTips2004,
  title = {The {{Linux}} Cookbook: Tips and Techniques for Everyday Use},
  shorttitle = {The {{Linux}} Cookbook},
  author = {Stutz, Michael},
  year = {2004},
  edition = {2nd ed., completely rev. and expanded},
  publisher = {No Starch Press},
  address = {San Francisco},
  isbn = {978-1-59327-031-5},
  langid = {english},
  lccn = {QA76.76.O63 S788 2004},
  keywords = {Linux,Operating systems (Computers)},
  file = {C:\Users\Dell\Zotero\storage\EEI8TSGW\Stutz - 2004 - The Linux cookbook tips and techniques for everyday use.pdf}
}

@misc{sunRetentiveNetworkSuccessor2023,
  title = {Retentive {{Network}}: {{A Successor}} to {{Transformer}} for {{Large Language Models}}},
  shorttitle = {Retentive {{Network}}},
  author = {Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
  year = {2023},
  month = aug,
  number = {arXiv:2307.08621},
  eprint = {2307.08621},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.08621},
  urldate = {2025-03-27},
  abstract = {In this work, we propose Retentive Network (RETNET) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost O(1) inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RETNET achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RETNET a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\Dell\Zotero\storage\EMQJY3WK\Sun et al. - 2023 - Retentive Network A Successor to Transformer for Large Language Models.pdf}
}

@misc{sutskeverSequenceSequenceLearning2014,
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  year = {2014},
  month = dec,
  number = {arXiv:1409.3215},
  eprint = {1409.3215},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1409.3215},
  urldate = {2025-03-27},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\Dell\Zotero\storage\VK92X6DH\Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf}
}

@book{thielZeroOneNotes2014,
  title = {{Zero to one: notes on startups, or how to build the future}},
  shorttitle = {{Zero to one}},
  author = {Thiel, Peter and Masters, Blake},
  year = {2014},
  edition = {1. Auflage},
  publisher = {Crown Business},
  address = {New York},
  isbn = {978-0-8041-3930-4 978-0-553-41828-6},
  langid = {german},
  file = {C:\Users\Dell\Zotero\storage\HRUYJGJD\Thiel and Masters - 2014 - Zero to one notes on startups, or how to build the future.epub}
}

@book{tifoHowWatchFootball2022,
  title = {How {{To Watch Football}}},
  author = {Tifo},
  year = {2022},
  month = oct,
  publisher = {Penguin Books},
  langid = {english},
  annotation = {Item ID: \_:n0},
  file = {C:\Users\Dell\Zotero\storage\QMRFH2WP\How To Watch Football -- Tifo -- 2, 1, 1, 2022 -- Penguin Books -- 9d5800e561d059dc36bfc6fde9ebc19e -- Anna’s Archive.epub}
}

@article{tysonInvestingDummies,
  title = {Investing for {{Dummies}}},
  author = {Tyson, Eric},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\52BXJQ82\Tyson - Investing for Dummies.pdf}
}

@misc{UnderstandingRecallHNSW,
  title = {Understanding {{Recall}} in {{HNSW Search}}},
  urldate = {2025-05-26},
  abstract = {Vector search systems, pivotal in AI applications, often rely on the Hierarchical Navigable Small Worlds (HNSW) algorithm. However, the behaviour of HNSW under real-world scenarios using vectors generated with deep learning models remains under-explored. This article focuses on HNSW's efficacy across a spectrum of datasets, including synthetic vectors tailored to mimic specific intrinsic dimensionalities, widely-used retrieval benchmarks with popular embedding models, and proprietary e-commerce image data with CLIP models. We survey the most popular HNSW vector databases and collate their default parameters to provide a realistic fixed parameterisation for the duration of the paper.},
  howpublished = {https://www.marqo.ai/blog/understanding-recall-in-hnsw-search},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\69UCYFIG\understanding-recall-in-hnsw-search.html}
}

@misc{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2023},
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.03762},
  urldate = {2025-03-27},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\Dell\Zotero\storage\ZQTYMQCL\Vaswani et al. - 2023 - Attention Is All You Need.pdf}
}

@book{vaughanImpracticalPythonProjects2019,
  title = {Impractical {{Python}} Projects: Playful Programming Activities to Make You Smarter},
  shorttitle = {Impractical {{Python}} Projects},
  author = {Vaughan, Lee},
  year = {2019},
  publisher = {No Starch Press},
  address = {San Francisco},
  abstract = {Impractical Python Projects picks up where the complete beginner books leave off, expanding on existing concepts and introducing new tools that you'll use every day. And to keep things interesting, each project includes a zany twist featuring historical incidents, pop culture references, and literary allusions. You'll flex your problem-solving skills and employ Python's many useful libraries to do things like: {$\bullet$} Help James Bond crack a high-tech safe with a hill-climbing algorithm{$\bullet$} Write haiku poems using Markov Chain Analysis {$\bullet$} Use genetic algorithms to breed a race of gigantic rats {$\bullet$} Crack the world's most successful military cipher using cryptanalysis {$\bullet$} Foil corporate security with invisible electronic ink {$\bullet$} Derive the anagram, "I am Lord Voldemort" using linguistical sieves {$\bullet$} Plan your parents' secure retirement with Monte Carlo simulation {$\bullet$} Save the sorceress Zatanna from a stabby death using palingrams {$\bullet$} Model the Milky Way and calculate our odds of detecting alien civilizations {$\bullet$} Help the world's smartest woman win the Monty Hall problem argument {$\bullet$} Reveal Jupiter's Great Red Spot using optical stacking {$\bullet$} Save the head of Mary, Queen of Scots with steganography Simulate volcanoes, map Mars, and more, all while gaining valuable experience using free modules like Tkinter, matplotlib, Cprofile, Pylint, Pygame, Pillow, and Python-Docx. Whether you're looking to pick up some new Python skills or just need a pick-me-up, you'll find endless educational, geeky fun with Impractical Python Projects},
  isbn = {978-1-59327-890-8},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\B5UW2RV6\Vaughan - 2019 - Impractical Python projects playful programming activities to make you smarter.pdf}
}

@book{yatesArtMemory2013,
  title = {Art {{Of Memory}}},
  author = {Yates, F. A.},
  year = {2013},
  publisher = {{Taylor and Francis}},
  address = {Hoboken},
  abstract = {First Published in 1999. Routledge is an imprint of Taylor \& Francis, an informa company},
  isbn = {978-0-415-22046-0},
  langid = {english},
  file = {C:\Users\Dell\Zotero\storage\FXWIZCQ5\Yates - 2013 - Art Of Memory.pdf}
}

@book{zingaroAlgorithmicThinkingProblembased2021,
  title = {Algorithmic Thinking: A Problem-Based Introduction},
  shorttitle = {Algorithmic Thinking},
  author = {Zingaro, Daniel},
  year = {2021},
  publisher = {No Starch Press},
  address = {San Francisco},
  abstract = {"An introduction to solving problems with algorithms and data structures, using competitive programming examples. Topics covered include recursion, dynamic programming, graphs, greedy algorithms, heaps, hash tables, segment trees, and other data structures for efficiently handling data"-Provided by publisher"--},
  isbn = {978-1-7185-0080-8},
  langid = {english},
  lccn = {QA76.9.A43 Z56 2021},
  keywords = {Computer algorithms,Computer programming,Problems exercises etc},
  file = {C:\Users\Dell\Zotero\storage\8JLDNKWS\Zingaro - 2021 - Algorithmic thinking a problem-based introduction.pdf}
}

@book{zingaroLearnCodeSolving2021,
  title = {Learn to Code by Solving Problems: A {{Python-based}} Introduction},
  shorttitle = {Learn to Code by Solving Problems},
  author = {Zingaro, Daniel},
  year = {2021},
  publisher = {No Starch Press},
  address = {San Francisco, CA},
  abstract = {"Teaches readers how to use Python to solve short, situational competitive programming problems. Each chapter requires the reader to learn a new feature or function of Python in order to solve a problem, while emphasizing basic programming concepts, problem-solving strategies, and critical thinking skills"--},
  isbn = {978-1-7185-0133-1},
  langid = {english},
  lccn = {QA76.73.P98},
  keywords = {Computer programming,Python (Computer program language)},
  file = {C:\Users\Dell\Zotero\storage\6QKIGEKE\Zingaro - 2021 - Learn to code by solving problems a Python-based introduction.pdf}
}
